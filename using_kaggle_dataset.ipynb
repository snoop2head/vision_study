{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "using_kaggle_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOIPU9X+ufQxHNrnSgRND72",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "116b7d903f0f49f4b799170a5096d66c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d296148ae36841f582f5a6ae73fc3cab",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_16ce930a0ce84058bad3ac32e65171d1",
              "IPY_MODEL_bb7a7f9c31b04ae3b291df4b7528148e"
            ]
          }
        },
        "d296148ae36841f582f5a6ae73fc3cab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16ce930a0ce84058bad3ac32e65171d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_15ec446149bd4fcaa211d84a3fa23a3e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 167502836,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 167502836,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d3e935ae5d8453d9bb056907ec42463"
          }
        },
        "bb7a7f9c31b04ae3b291df4b7528148e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7f12cd033af7414eab3ba733098053e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 160M/160M [00:25&lt;00:00, 6.56MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_798c4dcdfe294be3a51206284abfe1a3"
          }
        },
        "15ec446149bd4fcaa211d84a3fa23a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d3e935ae5d8453d9bb056907ec42463": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f12cd033af7414eab3ba733098053e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "798c4dcdfe294be3a51206284abfe1a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snoop2head/vision_study/blob/main/using_kaggle_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT4nJr8TElXW",
        "outputId": "6c11eff0-22d2-4c80-d146-99312a34cbc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.9)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KanKMK-EsHp"
      },
      "source": [
        "# make directory named ~/.kaggle\n",
        "!mkdir ~/.kaggle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jKZeobNEuWu"
      },
      "source": [
        "# move previously uploaded file into .kaggle directory\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvRlkuzTEvnt"
      },
      "source": [
        "# giving authorization for kaggle.json file\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmymPTywE5mq",
        "outputId": "1181c8cc-f654-41ac-828b-81c00fd7c39d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# look up datasets available in kaggle\n",
        "!kaggle datasets list"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4)\n",
            "ref                                                       title                                               size  lastUpdated          downloadCount  \n",
            "--------------------------------------------------------  -------------------------------------------------  -----  -------------------  -------------  \n",
            "terenceshin/covid19s-impact-on-airport-traffic            COVID-19's Impact on Airport Traffic               106KB  2020-10-19 12:40:17           1677  \n",
            "sootersaalu/amazon-top-50-bestselling-books-2009-2019     Amazon Top 50 Bestselling Books 2009 - 2019         15KB  2020-10-13 09:39:21           1640  \n",
            "thomaskonstantin/highly-rated-children-books-and-stories  Highly Rated Children Books And Stories            106KB  2020-10-24 12:09:59            354  \n",
            "tunguz/euro-parliament-proceedings-1996-2011              Euro Parliament Proceedings 1996 - 2011              1GB  2020-10-26 17:48:29             22  \n",
            "rishidamarla/judicial-expenditures-across-all-50-states   Judicial Expenditures across all 50 States           2KB  2020-10-25 00:07:45            216  \n",
            "docstein/brics-world-bank-indicators                      BRICS World Bank Indicators                          4MB  2020-10-22 12:18:40            363  \n",
            "kanishk307/6000-indian-food-recipes-dataset               6000+ Indian Food Recipes Dataset                    9MB  2020-10-24 01:08:23            438  \n",
            "elvinagammed/chatbots-intent-recognition-dataset          Chatbots: Intent Recognition Dataset                17KB  2020-10-23 07:44:59            189  \n",
            "omarhanyy/500-greatest-songs-of-all-time                  500 Greatest Songs of All Time                      33KB  2020-10-26 13:36:09            430  \n",
            "balraj98/synthetic-objective-testing-set-sots-reside      Synthetic Objective Testing Set (SOTS) [RESIDE]    415MB  2020-10-24 10:07:29             53  \n",
            "lunamcbride24/pokemon-type-matchup-data                   Pokemon Type Matchup Data                            9KB  2020-10-14 18:56:23            262  \n",
            "gaurav2796/kaggle-competions-rankings-and-kernels         Kaggle Competions, Rankings and Kernels            698KB  2020-10-15 04:05:15             32  \n",
            "balraj98/indoor-training-set-its-residestandard           Indoor Training Set (ITS) [RESIDE-Standard]          5GB  2020-10-24 10:07:30             35  \n",
            "romazepa/moscow-schools-winners-of-educational-olympiads  Moscow schools - winners of educational Olympiads    1MB  2020-10-12 21:45:01             56  \n",
            "sootersaalu/nigerian-songs-spotify                        Nigerian Songs Spotify                              24KB  2020-10-25 19:10:23             60  \n",
            "salmaneunus/mechanical-tools-dataset                      Mechanical Tools Classification Dataset            652MB  2020-11-01 11:28:22            170  \n",
            "thanatoz/hinglish-blogs                                   Hinglish blogs                                       2MB  2020-10-13 18:16:05             12  \n",
            "shivamb/netflix-shows                                     Netflix Movies and TV Shows                        971KB  2020-01-20 07:33:56          52885  \n",
            "nehaprabhavalkar/indian-food-101                          Indian Food 101                                      7KB  2020-09-30 06:23:43           5945  \n",
            "heeraldedhia/groceries-dataset                            Groceries dataset                                  257KB  2020-09-17 04:36:08           6207  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJKwaeY_E7Ot",
        "outputId": "30b6308d-57a7-41b6-ca24-d81d28dfeec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# download competition dataset with url query\n",
        "!kaggle competitions download -c 'global-wheat-detection'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4)\n",
            "Downloading 010dbcc8f.jpg to /content\n",
            "  0% 0.00/107k [00:00<?, ?B/s]\n",
            "100% 107k/107k [00:00<00:00, 42.0MB/s]\n",
            "Downloading 015939012.jpg to /content\n",
            "  0% 0.00/132k [00:00<?, ?B/s]\n",
            "100% 132k/132k [00:00<00:00, 44.3MB/s]\n",
            "Downloading 00e903abe.jpg to /content\n",
            "  0% 0.00/300k [00:00<?, ?B/s]\n",
            "100% 300k/300k [00:00<00:00, 97.3MB/s]\n",
            "Downloading 006a994f7.jpg to /content\n",
            "  0% 0.00/110k [00:00<?, ?B/s]\n",
            "100% 110k/110k [00:00<00:00, 113MB/s]\n",
            "Downloading 01397a84c.jpg to /content\n",
            "  0% 0.00/156k [00:00<?, ?B/s]\n",
            "100% 156k/156k [00:00<00:00, 50.9MB/s]\n",
            "Downloading 00764ad5d.jpg to /content\n",
            "  0% 0.00/135k [00:00<?, ?B/s]\n",
            "100% 135k/135k [00:00<00:00, 121MB/s]\n",
            "Downloading 00b5c6764.jpg to /content\n",
            "  0% 0.00/173k [00:00<?, ?B/s]\n",
            "100% 173k/173k [00:00<00:00, 58.4MB/s]\n",
            "Downloading 010b216d4.jpg to /content\n",
            "  0% 0.00/189k [00:00<?, ?B/s]\n",
            "100% 189k/189k [00:00<00:00, 61.9MB/s]\n",
            "Downloading 013fd7d80.jpg to /content\n",
            "  0% 0.00/141k [00:00<?, ?B/s]\n",
            "100% 141k/141k [00:00<00:00, 46.5MB/s]\n",
            "Downloading 013669953.jpg to /content\n",
            "  0% 0.00/212k [00:00<?, ?B/s]\n",
            "100% 212k/212k [00:00<00:00, 68.8MB/s]\n",
            "Downloading 00b5fefed.jpg to /content\n",
            "  0% 0.00/130k [00:00<?, ?B/s]\n",
            "100% 130k/130k [00:00<00:00, 44.0MB/s]\n",
            "Downloading 00ea5e5ee.jpg to /content\n",
            "  0% 0.00/179k [00:00<?, ?B/s]\n",
            "100% 179k/179k [00:00<00:00, 56.0MB/s]\n",
            "Downloading 0114c88aa.jpg to /content\n",
            "  0% 0.00/198k [00:00<?, ?B/s]\n",
            "100% 198k/198k [00:00<00:00, 61.5MB/s]\n",
            "Downloading 005b0d8bb.jpg to /content\n",
            "  0% 0.00/142k [00:00<?, ?B/s]\n",
            "100% 142k/142k [00:00<00:00, 44.7MB/s]\n",
            "Downloading 0126b7d11.jpg to /content\n",
            "  0% 0.00/237k [00:00<?, ?B/s]\n",
            "100% 237k/237k [00:00<00:00, 78.2MB/s]\n",
            "Downloading 0172359d2.jpg to /content\n",
            "  0% 0.00/309k [00:00<?, ?B/s]\n",
            "100% 309k/309k [00:00<00:00, 89.7MB/s]\n",
            "Downloading 01189a3c3.jpg to /content\n",
            "  0% 0.00/260k [00:00<?, ?B/s]\n",
            "100% 260k/260k [00:00<00:00, 85.6MB/s]\n",
            "Downloading 010c93b99.jpg to /content\n",
            "  0% 0.00/126k [00:00<?, ?B/s]\n",
            "100% 126k/126k [00:00<00:00, 115MB/s]\n",
            "Downloading 00333207f.jpg to /content\n",
            "  0% 0.00/255k [00:00<?, ?B/s]\n",
            "100% 255k/255k [00:00<00:00, 82.4MB/s]\n",
            "Downloading 00b70a919.jpg to /content\n",
            "  0% 0.00/137k [00:00<?, ?B/s]\n",
            "100% 137k/137k [00:00<00:00, 121MB/s]\n",
            "Downloading cc3532ff6.jpg to /content\n",
            "  0% 0.00/190k [00:00<?, ?B/s]\n",
            "100% 190k/190k [00:00<00:00, 61.7MB/s]\n",
            "Downloading f5a1f0358.jpg to /content\n",
            "  0% 0.00/187k [00:00<?, ?B/s]\n",
            "100% 187k/187k [00:00<00:00, 61.2MB/s]\n",
            "Downloading aac893a91.jpg to /content\n",
            "  0% 0.00/214k [00:00<?, ?B/s]\n",
            "100% 214k/214k [00:00<00:00, 63.0MB/s]\n",
            "Downloading 53f253011.jpg to /content\n",
            "  0% 0.00/169k [00:00<?, ?B/s]\n",
            "100% 169k/169k [00:00<00:00, 54.3MB/s]\n",
            "Downloading 51f1be19e.jpg to /content\n",
            "  0% 0.00/252k [00:00<?, ?B/s]\n",
            "100% 252k/252k [00:00<00:00, 83.0MB/s]\n",
            "Downloading cb8d261a3.jpg to /content\n",
            "  0% 0.00/140k [00:00<?, ?B/s]\n",
            "100% 140k/140k [00:00<00:00, 126MB/s]\n",
            "Downloading 51b3e36ab.jpg to /content\n",
            "  0% 0.00/188k [00:00<?, ?B/s]\n",
            "100% 188k/188k [00:00<00:00, 60.9MB/s]\n",
            "Downloading 2fd875eaa.jpg to /content\n",
            "  0% 0.00/144k [00:00<?, ?B/s]\n",
            "100% 144k/144k [00:00<00:00, 129MB/s]\n",
            "Downloading 796707dd7.jpg to /content\n",
            "  0% 0.00/180k [00:00<?, ?B/s]\n",
            "100% 180k/180k [00:00<00:00, 58.6MB/s]\n",
            "Downloading 348a992bb.jpg to /content\n",
            "  0% 0.00/176k [00:00<?, ?B/s]\n",
            "100% 176k/176k [00:00<00:00, 57.3MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/266 [00:00<?, ?B/s]\n",
            "100% 266/266 [00:00<00:00, 273kB/s]\n",
            "Downloading train.csv.zip to /content\n",
            "  0% 0.00/1.29M [00:00<?, ?B/s]\n",
            "100% 1.29M/1.29M [00:00<00:00, 88.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuTAGGhNFG1g"
      },
      "source": [
        "!mkdir test\n",
        "!mkdir train"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohtAcr1VFg8b",
        "outputId": "5f6b4b9e-26ef-4464-f017-cf6c9caf6c0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip train.csv.zip -d train"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.csv.zip\n",
            "  inflating: train/train.csv         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmOGOFy4GuVc",
        "outputId": "6d27a060-e411-4a86-c772-0c1285e89cfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -U git+https://github.com/albu/albumentations > /dev/null && echo \"All libraries are successfully installed!\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-prwzyjav\n",
            "All libraries are successfully installed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uliaFc1hGzpP"
      },
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUto_8ErFj-6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import re\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "DIR_INPUT = '.'\n",
        "DIR_TRAIN = f'{DIR_INPUT}/train'\n",
        "DIR_TEST = f'{DIR_INPUT}/test'\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waSMcoXAF9t9",
        "outputId": "0a3fded7-9686-44f0-e113-254a57ff2adf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_df = pd.read_csv(f'{DIR_TRAIN}/train.csv')\n",
        "train_df.shape\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(147793, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UenWZpwcGByC"
      },
      "source": [
        "train_df['x'] = -1\n",
        "train_df['y'] = -1\n",
        "train_df['w'] = -1\n",
        "train_df['h'] = -1\n",
        "\n",
        "def expand_bbox(x):\n",
        "    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n",
        "    if len(r) == 0:\n",
        "        r = [-1, -1, -1, -1]\n",
        "    return r\n",
        "\n",
        "train_df[['x', 'y', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\n",
        "train_df.drop(columns=['bbox'], inplace=True)\n",
        "train_df['x'] = train_df['x'].astype(np.float)\n",
        "train_df['y'] = train_df['y'].astype(np.float)\n",
        "train_df['w'] = train_df['w'].astype(np.float)\n",
        "train_df['h'] = train_df['h'].astype(np.float)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80uQfNstGHjg"
      },
      "source": [
        "image_ids = train_df['image_id'].unique()\n",
        "valid_ids = image_ids[-665:]\n",
        "train_ids = image_ids[:-665]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aU-BRzYGJrE"
      },
      "source": [
        "valid_df = train_df[train_df['image_id'].isin(valid_ids)]\n",
        "train_df = train_df[train_df['image_id'].isin(train_ids)]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A6v4gCPGKtT",
        "outputId": "b411cdde-a3a0-4f0c-fbbb-b52c5e252819",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "valid_df.shape, train_df.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25006, 8), (122787, 8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1im3iU0FGLhM"
      },
      "source": [
        "class WheatDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, image_dir, transforms=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_ids = dataframe['image_id'].unique()\n",
        "        self.df = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "\n",
        "        image_id = self.image_ids[index]\n",
        "        records = self.df[self.df['image_id'] == image_id]\n",
        "\n",
        "        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "\n",
        "        boxes = records[['x', 'y', 'w', 'h']].values\n",
        "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
        "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
        "        \n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        area = torch.as_tensor(area, dtype=torch.float32)\n",
        "\n",
        "        # there is only one class\n",
        "        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
        "        \n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
        "        \n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels\n",
        "        # target['masks'] = None\n",
        "        target['image_id'] = torch.tensor([index])\n",
        "        target['area'] = area\n",
        "        target['iscrowd'] = iscrowd\n",
        "\n",
        "        if self.transforms:\n",
        "            sample = {\n",
        "                'image': image,\n",
        "                'bboxes': target['boxes'],\n",
        "                'labels': labels\n",
        "            }\n",
        "            sample = self.transforms(**sample)\n",
        "            image = sample['image']\n",
        "            \n",
        "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "\n",
        "        return image, target, image_id\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.image_ids.shape[0]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V5OkF09GNbs"
      },
      "source": [
        "# Albumentations\n",
        "def get_train_transform():\n",
        "    return A.Compose([\n",
        "        A.Flip(0.5),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "\n",
        "def get_valid_transform():\n",
        "    return A.Compose([\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpVCBZU6GRfO",
        "outputId": "4a10f378-ffce-415f-ee92-f969afdca048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "116b7d903f0f49f4b799170a5096d66c",
            "d296148ae36841f582f5a6ae73fc3cab",
            "16ce930a0ce84058bad3ac32e65171d1",
            "bb7a7f9c31b04ae3b291df4b7528148e",
            "15ec446149bd4fcaa211d84a3fa23a3e",
            "4d3e935ae5d8453d9bb056907ec42463",
            "7f12cd033af7414eab3ba733098053e8",
            "798c4dcdfe294be3a51206284abfe1a3"
          ]
        }
      },
      "source": [
        "# load a model; pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "116b7d903f0f49f4b799170a5096d66c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=167502836.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSSVbgbWGTBN"
      },
      "source": [
        "num_classes = 2  # 1 class (wheat) + background\n",
        "\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeGIToiVGVtU"
      },
      "source": [
        "class Averager:\n",
        "    def __init__(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0\n",
        "\n",
        "    def send(self, value):\n",
        "        self.current_total += value\n",
        "        self.iterations += 1\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        if self.iterations == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1.0 * self.current_total / self.iterations\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjAM5csvGX00"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_dataset = WheatDataset(train_df, DIR_TRAIN, get_train_transform())\n",
        "valid_dataset = WheatDataset(valid_df, DIR_TRAIN, get_valid_transform())\n",
        "\n",
        "\n",
        "# split the dataset in train and test set\n",
        "indices = torch.randperm(len(train_dataset)).tolist()\n",
        "\n",
        "train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "valid_data_loader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6G4YkD8GZVH"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZSjq_enJEIR",
        "outputId": "6765e8b6-3f2c-42d1-a3c3-9bd842d252d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "source": [
        "images, targets, image_ids = next(iter(train_data_loader))\n",
        "images = list(image.to(device) for image in images)\n",
        "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-1a879a574a92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31merror\u001b[0m: Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-18-8ee0cb15600c>\", line 17, in __getitem__\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\ncv2.error: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9MPuhiSJFcQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}